{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "\n",
    "#------------------- def methods -----------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types\n",
      "Train data and labels\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "Test data and labels\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "Rank and dimensionality for each axis\n",
      "- train data:\n",
      "2\n",
      "(404, 13)\n",
      "- train labels:\n",
      "1\n",
      "(404,)\n",
      "\n",
      "- test data\n",
      "2\n",
      "(102, 13)\n",
      "- test labels:\n",
      "1\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "# load data and analyze the type, rank and dimensionality for each rank \n",
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()\n",
    "\n",
    "print('Data types')\n",
    "print('Train data and labels')\n",
    "print(type(train_data))\n",
    "print(type(train_labels))\n",
    "print('\\nTest data and labels')\n",
    "print(type(test_data))\n",
    "print(type(test_labels))\n",
    "\n",
    "print('\\nRank and dimensionality for each axis')\n",
    "print('- train data:')\n",
    "print(train_data.ndim)\n",
    "print(train_data.shape)\n",
    "\n",
    "print('- train labels:')\n",
    "print(train_labels.ndim)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "print('\\n- test data')\n",
    "print(test_data.ndim)\n",
    "print(test_data.shape)\n",
    "\n",
    "print('- test labels:')\n",
    "print(test_labels.ndim)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# having few data in training data involves that validation should be applied in a different way than done\n",
    "# in binary and multiclassification, for instance using k-fold validation. Moreover the neural network shouldn't be so complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data content\n",
      "[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ]  - label:  15.2\n",
      "\n",
      "Test data content\n",
      "[ 18.0846   0.      18.1      0.       0.679    6.434  100.       1.8347\n",
      "  24.     666.      20.2     27.25    29.05  ]  - label:  7.2\n"
     ]
    }
   ],
   "source": [
    "print('Train data content')\n",
    "print(train_data[0], ' - label: ', train_labels[0])\n",
    "\n",
    "print('\\nTest data content')\n",
    "print(test_data[0], ' - label: ', test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is in a good format (tensor) so we don't need to apply transformations\n",
    "# However we need to apply feature normalization because the values printed above are expressed on different numerical scales\n",
    "# train_data[0][2] << train_data[0][11]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data content\n",
      "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]  - label:  15.2\n",
      "\n",
      "Test data content\n",
      "[ 1.55369355 -0.48361547  1.0283258  -0.25683275  1.03838067  0.23545815\n",
      "  1.11048828 -0.93976936  1.67588577  1.5652875   0.78447637 -3.48459553\n",
      "  2.25092074]  - label:  7.2\n"
     ]
    }
   ],
   "source": [
    "# We print again the data modified\n",
    "\n",
    "print('Train data content')\n",
    "print(train_data[0], ' - label: ', train_labels[0])\n",
    "\n",
    "print('\\nTest data content')\n",
    "print(test_data[0], ' - label: ', test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# we an build our NN considering the problem we are going to face\n",
    "\n",
    "# model definition \n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(13,))) # 13 features, see the x dimensionality for the training_data tensor \n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "from keras import optimizers\n",
    "          \n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse', \n",
    "              metrics=['mae']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data per fold:  134\n",
      "# ---------- #\n",
      "Current fold:  0\n",
      "First index val set:  0    Last index val set:  134\n",
      "Dim tmpTrainData: x ->  270    y ->  13\n",
      "Dim tmpTrainLabels: y ->  270\n",
      "# ---------- #\n",
      "Current fold:  1\n",
      "First index val set:  134    Last index val set:  268\n",
      "Dim tmpTrainData: x ->  270    y ->  13\n",
      "Dim tmpTrainLabels: y ->  270\n",
      "# ---------- #\n",
      "Current fold:  2\n",
      "First index val set:  268    Last index val set:  404\n",
      "Dim tmpTrainData: x ->  268    y ->  13\n",
      "Dim tmpTrainLabels: y ->  268\n"
     ]
    }
   ],
   "source": [
    "# Validation should be applied using k-fold approach because the data we could include in validation couldn't be\n",
    "# generalize the data \n",
    "# ...\n",
    "\n",
    "num_epochs = 20\n",
    "k = 3 # Number of folds\n",
    "dataPerFold = train_data.shape[0] // k\n",
    "print(\"Data per fold: \", dataPerFold)\n",
    "for tmpK in range(k): # 0 -> 2\n",
    "    print(\"# ---------- #\")\n",
    "    print(\"Current fold: \", tmpK)\n",
    "    if (tmpK < (k - 1)):\n",
    "        lastIndex = (tmpK+1) * dataPerFold\n",
    "    else:\n",
    "        lastIndex = len(train_data)\n",
    "    tmpValidationSet = train_data[tmpK * dataPerFold : lastIndex]\n",
    "    tmpValidationLabels = train_labels[tmpK * dataPerFold : lastIndex]\n",
    "    print(\"First index val set: \", tmpK * dataPerFold, \"   Last index val set: \", lastIndex)\n",
    "    #tmpTrainData doesn't contain the records included in range [k * dataPerFold, (k+1) * dataPerFold] or [k * dataPerFold, len(train_data)]\n",
    "    #Now I create the temporary train_data and temporary train_labels\n",
    "    tmpTrainData = np.zeros((train_data.shape[0] - (lastIndex - (tmpK * dataPerFold)), train_data.shape[1]))\n",
    "    tmpTrainLabels = np.zeros((train_data.shape[0] - (lastIndex - (tmpK * dataPerFold))))\n",
    "    print(\"Dim tmpTrainData: x -> \", tmpTrainData.shape[0], \"   y -> \", tmpTrainData.shape[1])\n",
    "    print(\"Dim tmpTrainLabels: y -> \", tmpTrainLabels.shape[0])\n",
    "    for tmpIndex in range(tmpTrainData.shape[0]):\n",
    "        for i in range(train_data.shape[0]):   \n",
    "            if ((i < (k * dataPerFold)) or (i >= lastIndex)): \n",
    "                # train_data[i] added to tmpTrainData\n",
    "                tmpTrainData[tmpIndex] = train_data[i]\n",
    "                tmpTrainLabels[tmpIndex] = train_labels[i]\n",
    "    '''\n",
    "    # tmpTrainData tmpTrainLabels tmpValidationSet tmpValidationLabels\n",
    "    # Here I use tmpTrainData and tmpTrainLabels for fitting and collecting the errors values\n",
    "    tmpModel = createModel() # TO DO\n",
    "    fitStatistics = tmpModel.fit() # TO DO\n",
    "    # array of array\n",
    "    all_mae_histories.append(fitStatistics.history['val_mean_absolute_error']) # == [epoch1 = n, epoch2 = m, ..., epoch20 = z]\n",
    "    \n",
    "    \n",
    "# compute the mean value for each epoch in each fold\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "# TO DO -> plot (epochs, all_mae_histories)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Saverio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "404/404 [==============================] - 0s 387us/step - loss: 508.4762 - mae: 20.5895\n",
      "Epoch 2/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 333.2186 - mae: 16.0513\n",
      "Epoch 3/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 159.4215 - mae: 10.2101\n",
      "Epoch 4/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 72.6914 - mae: 6.4141\n",
      "Epoch 5/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 44.5636 - mae: 4.9101\n",
      "Epoch 6/20\n",
      "404/404 [==============================] - 0s 77us/step - loss: 33.2512 - mae: 4.1602\n",
      "Epoch 7/20\n",
      "404/404 [==============================] - 0s 193us/step - loss: 27.5553 - mae: 3.7514\n",
      "Epoch 8/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 24.3870 - mae: 3.4469\n",
      "Epoch 9/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 22.0656 - mae: 3.3415\n",
      "Epoch 10/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 20.0836 - mae: 3.1609\n",
      "Epoch 11/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 18.7636 - mae: 3.0128\n",
      "Epoch 12/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 16.9121 - mae: 2.8833\n",
      "Epoch 13/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 15.8594 - mae: 2.7830\n",
      "Epoch 14/20\n",
      "404/404 [==============================] - 0s 193us/step - loss: 14.7898 - mae: 2.6516\n",
      "Epoch 15/20\n",
      "404/404 [==============================] - 0s 77us/step - loss: 14.1812 - mae: 2.6507\n",
      "Epoch 16/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 12.9769 - mae: 2.5399\n",
      "Epoch 17/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 12.7834 - mae: 2.4991\n",
      "Epoch 18/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 12.0409 - mae: 2.4027\n",
      "Epoch 19/20\n",
      "404/404 [==============================] - 0s 77us/step - loss: 11.6233 - mae: 2.4093\n",
      "Epoch 20/20\n",
      "404/404 [==============================] - 0s 116us/step - loss: 11.4538 - mae: 2.3819\n"
     ]
    }
   ],
   "source": [
    "fitStatistics = model.fit(train_data, train_labels, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 460us/step\n",
      "[21.39389741187002, 3.1629059314727783]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(test_data, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
